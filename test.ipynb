{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral.model import Transformer , ModelArgs\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../mar-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Array:\n",
      "[['<unk>', 0.0], ['<s>', 0.0], ['</s>', 0.0], ['े', -3.198136806488037], ['ी', -3.234046697616577], ['▁', -3.6190669536590576], ['व', -3.791450023651123], ['ा', -3.8097009658813477], ['ल', -3.8552277088165283], ['र', -4.030426979064941], ['▁क', -4.096604347229004], ['.', -4.098381519317627], ['▁आहे', -4.098432540893555], ['न', -4.101080417633057], ['ो', -4.182425022125244], ['श', -4.188457012176514], ['च', -4.234373092651367], ['▁स', -4.389428615570068], [',', -4.434492588043213], ['्या', -4.54027795791626], ['त', -4.6261701583862305], ['ड', -4.655073642730713], ['ही', -4.68520975112915], ['▁केल', -4.7505269050598145], ['ू', -4.913422107696533], ['▁प', -4.955512046813965], ['ाठ', -4.957014083862305], ['म', -5.010734558105469], ['स', -5.011702537536621], ['्य', -5.055701732635498], ['ण', -5.057131290435791], ['ाल', -5.062128067016602], ['क', -5.07785177230835], ['ग', -5.083362102508545], ['▁म', -5.091159343719482], ['▁न', -5.118353843688965], ['▁अ', -5.156129837036133], ['▁मीडिया', -5.194016456604004], ['▁पोलिस', -5.194025039672852], ['▁प्र', -5.19560432434082], ['ण्यास', -5.1956562995910645], ['त्या', -5.203145980834961], ['ि', -5.2032952308654785], ['▁वि', -5.22069787979126], ['▁अस', -5.25075626373291], ['▁ल', -5.257070541381836], ['वा', -5.331004619598389], ['▁व', -5.332678318023682], ['ात', -5.338829517364502], ['द', -5.357605457305908], ['ं', -5.371214389801025], ['ु', -5.396023750305176], ['ला', -5.41054105758667], ['ब', -5.423957824707031], ['ज', -5.480068206787109], ['ोण', -5.48902702331543], ['-', -5.527349948883057], ['णाऱ्या', -5.527349948883057], ['ळ', -5.527349948883057], ['▁संघटनांनी', -5.527349948883057], ['▁उ', -5.527351379394531], ['्याच', -5.530144691467285], ['क्', -5.535965919494629], ['ांनी', -5.5379133224487305], ['▁प्रकारच', -5.5380449295043945], ['▁कार', -5.556464195251465], ['टर', -5.569016933441162], ['▁मा', -5.577760219573975], ['र्', -5.59059476852417], ['ोर', -5.591390132904053], ['▁ग', -5.593201160430908], ['ंत', -5.612587928771973], ['▁आ', -5.651533603668213], ['िं', -5.659958839416504], ['▁च', -5.682860851287842], ['ासन', -5.695425510406494], ['▁की', -5.698838710784912], ['्ह', -5.711410045623779], ['ह', -5.771453857421875], ['ती', -5.7780327796936035], ['हि', -5.8039422035217285], ['ट', -5.837793350219727], ['मध', -5.859158515930176], ['ठ', -5.877134323120117], ['ध', -5.888587951660156], ['णा', -5.897660255432129], ['ील', -5.973944187164307], ['प', -5.977757453918457], ['अ', -5.990391254425049], ['▁ह', -5.994476318359375], ['▁शे', -6.027280330657959], ['ख', -6.027349948883057], ['घ', -6.027349948883057], ['झ', -6.027349948883057], ['४', -6.027349948883057], ['▁एटीएम', -6.027349948883057], ['▁१', -6.027349948883057], ['ाई', -6.027351379394531], ['ॅ', -6.027352333068848], ['▁आवाहन', -6.027352333068848], ['दर्शन', -6.027353763580322], ['ट्रॉली', -6.027354717254639], ['▁पसरव', -6.027462005615234], ['▁दिल', -6.027617931365967], ['भाग', -6.028197765350342], ['▁राज्यात', -6.02960729598999], ['द्', -6.029763221740723], ['▁धा', -6.031162738800049], ['▁करू', -6.033496856689453], ['▁बं', -6.03418493270874], ['क्षण', -6.035065174102783], ['वि', -6.038300514221191], ['▁शेत', -6.0409746170043945], ['ुक', -6.049130439758301], ['ट्र', -6.057026386260986], ['्थ', -6.0699238777160645], ['▁मो', -6.087465763092041], ['▁या', -6.088471412658691], ['टी', -6.092829704284668], ['िल', -6.092902183532715], ['▁ब', -6.103559494018555], ['▁ज', -6.1095662117004395], ['ली', -6.110528945922852], ['कां', -6.129710674285889], ['बर', -6.129773139953613], ['▁हो', -6.151933670043945], ['▁आर', -6.177620887756348], ['गत', -6.246683597564697], ['▁आंदोलना', -6.251964569091797], ['दी', -6.256585121154785], ['मु', -6.258923530578613], ['शा', -6.26564359664917], ['कू', -6.268863201141357], ['▁ठ', -6.274503231048584], ['▁आल', -6.3119378089904785], ['ाग', -6.330749034881592], ['नि', -6.356349468231201], ['ाय', -6.3879780769348145], ['ंनी', -6.399325847625732], ['▁त्यां', -6.416606426239014], ['ध्य', -6.420991897583008], ['िक', -6.465760707855225], ['ेश', -6.469394683837891], ['या', -6.553012847900391], ['ते', -6.574105262756348], ['चा', -6.596765041351318], ['▁आंदोलन', -6.6046600341796875], ['्यात', -6.617892742156982], ['रि', -6.644131183624268], ['ाच', -6.677464962005615], ['ूर', -6.73527193069458], ['▁का', -6.744552135467529], ['तच', -6.784526348114014], ['्यम', -6.786521911621094], ['ठे', -6.806637763977051], ['गि', -6.81339693069458], ['▁त्या', -6.8422040939331055], ['रा', -6.868304252624512], ['ंव', -6.873676300048828], ['नी', -6.898252487182617], ['था', -6.925732135772705], ['का', -6.9539947509765625], ['तह', -6.962111473083496], ['ार', -6.965326309204102], ['▁प्रकार', -6.984320163726807], ['क्ष', -7.0079121589660645], ['▁राज्य', -7.021665573120117], ['भ', -7.02518892288208], ['ट्', -7.025625228881836], ['रकारच', -7.027314186096191], ['ॉ', -7.027339458465576], [':', -7.027349948883057], ['इ', -7.027349948883057], ['ओ', -7.027349948883057], ['औ', -7.027349948883057], ['ढ', -7.027349948883057], ['फ', -7.027349948883057], ['५', -7.027349948883057], ['ै', -7.027353286743164], ['प्रद', -7.0275044441223145], ['त्', -7.031057357788086], ['प्', -7.04274320602417], ['▁पोल', -7.04673433303833], ['▁करण', -7.052454948425293], ['सा', -7.072572231292725], ['्यान', -7.072592258453369], ['से', -7.076502323150635], ['्र', -7.089873790740967], ['डि', -7.123448371887207], ['्यां', -7.135494709014893], ['्', -7.29254674911499], ['य', -7.817433834075928], ['१', -8.385090827941895], ['ई', -8.385190963745117], ['ष', -8.38529109954834], ['ऱ', -8.385391235351562], ['थ', -8.385490417480469], ['उ', -8.385590553283691], ['ए', -8.385690689086914], ['आ', -8.385790824890137]]\n",
      "\n",
      "Count of Items in Vocabulary Array: 200\n"
     ]
    }
   ],
   "source": [
    "with open('../../mar-tokenizer/tokenizer.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Access the 'model' key and its 'vocab' array\n",
    "vocab_array = data['model']['vocab']\n",
    "\n",
    "# Count the items inside the 'vocab' array\n",
    "vocab_count = len(vocab_array)\n",
    "\n",
    "# Print the 'vocab' array and its count\n",
    "print(\"Vocabulary Array:\")\n",
    "print(vocab_array)\n",
    "\n",
    "print(\"\\nCount of Items in Vocabulary Array:\", vocab_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_loaded = LlamaTokenizerFast.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_loaded(\"डिसेंबर ह्या दिवशी श्रीकृष्णाने अर्जुनाला गीता सांगितली.\")\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = ModelArgs(dim=1024,\n",
    "                n_layers=12,\n",
    "                head_dim=6,\n",
    "                hidden_dim=4096,\n",
    "                n_heads=8,\n",
    "                n_kv_heads=4,\n",
    "                norm_eps=1e-6,\n",
    "                max_batch_size=8,\n",
    "                vocab_size=200,\n",
    "                sliding_window=4,\n",
    "                )\n",
    "\n",
    "\n",
    "# arg = ModelArgs(dim=1024,\n",
    "#                 n_layers=6,\n",
    "#                 head_dim=4,\n",
    "#                 hidden_dim=4096,\n",
    "#                 n_heads=6,\n",
    "#                 n_kv_heads=2,\n",
    "#                 norm_eps=1e-6,\n",
    "#                 max_batch_size=8,\n",
    "#                 vocab_size=200,\n",
    "#                 sliding_window=4,\n",
    "#                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim= 1024\n",
    "n_layers= 6\n",
    "head_dim= 4\n",
    "hidden_dim= 512\n",
    "n_heads= 4\n",
    "n_kv_heads= 2\n",
    "norm_eps= 1e-6\n",
    "vocab_size= 30000\n",
    "\n",
    "max_batch_size= 2\n",
    "\n",
    "# For rotary embeddings. If not set, will be infered from sliding window.\n",
    "rope_theta=  None\n",
    "# If this is set, use sliding window attention rotating cache.\n",
    "sliding_window=  None\n",
    "# If this is set, we will use MoE layers instead of dense layers.\n",
    "moe=  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(args=arg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight: torch.Size([200, 1024])\n",
      "norm.weight: torch.Size([1024])\n",
      "output.weight: torch.Size([200, 1024])\n",
      "layers.0.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.0.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.0.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.0.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.0.attention_norm.weight: torch.Size([1024])\n",
      "layers.0.ffn_norm.weight: torch.Size([1024])\n",
      "layers.0.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.0.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.0.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.1.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.1.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.1.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.1.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.1.attention_norm.weight: torch.Size([1024])\n",
      "layers.1.ffn_norm.weight: torch.Size([1024])\n",
      "layers.1.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.1.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.1.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.2.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.2.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.2.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.2.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.2.attention_norm.weight: torch.Size([1024])\n",
      "layers.2.ffn_norm.weight: torch.Size([1024])\n",
      "layers.2.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.2.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.2.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.3.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.3.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.3.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.3.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.3.attention_norm.weight: torch.Size([1024])\n",
      "layers.3.ffn_norm.weight: torch.Size([1024])\n",
      "layers.3.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.3.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.3.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.4.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.4.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.4.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.4.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.4.attention_norm.weight: torch.Size([1024])\n",
      "layers.4.ffn_norm.weight: torch.Size([1024])\n",
      "layers.4.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.4.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.4.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.5.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.5.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.5.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.5.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.5.attention_norm.weight: torch.Size([1024])\n",
      "layers.5.ffn_norm.weight: torch.Size([1024])\n",
      "layers.5.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.5.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.5.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.6.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.6.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.6.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.6.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.6.attention_norm.weight: torch.Size([1024])\n",
      "layers.6.ffn_norm.weight: torch.Size([1024])\n",
      "layers.6.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.6.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.6.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.7.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.7.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.7.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.7.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.7.attention_norm.weight: torch.Size([1024])\n",
      "layers.7.ffn_norm.weight: torch.Size([1024])\n",
      "layers.7.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.7.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.7.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.8.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.8.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.8.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.8.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.8.attention_norm.weight: torch.Size([1024])\n",
      "layers.8.ffn_norm.weight: torch.Size([1024])\n",
      "layers.8.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.8.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.8.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.9.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.9.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.9.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.9.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.9.attention_norm.weight: torch.Size([1024])\n",
      "layers.9.ffn_norm.weight: torch.Size([1024])\n",
      "layers.9.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.9.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.9.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.10.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.10.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.10.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.10.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.10.attention_norm.weight: torch.Size([1024])\n",
      "layers.10.ffn_norm.weight: torch.Size([1024])\n",
      "layers.10.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.10.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.10.feed_forward.w3.weight: torch.Size([4096, 1024])\n",
      "layers.11.attention.wq.weight: torch.Size([48, 1024])\n",
      "layers.11.attention.wk.weight: torch.Size([24, 1024])\n",
      "layers.11.attention.wv.weight: torch.Size([24, 1024])\n",
      "layers.11.attention.wo.weight: torch.Size([1024, 48])\n",
      "layers.11.attention_norm.weight: torch.Size([1024])\n",
      "layers.11.ffn_norm.weight: torch.Size([1024])\n",
      "layers.11.feed_forward.w1.weight: torch.Size([4096, 1024])\n",
      "layers.11.feed_forward.w2.weight: torch.Size([1024, 4096])\n",
      "layers.11.feed_forward.w3.weight: torch.Size([4096, 1024])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 584.41 MB\n"
     ]
    }
   ],
   "source": [
    "model_size_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Model size: {model_size_bytes / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(3, 128).unsqueeze(0)\n",
    "x = torch.rand(2, 2, 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 128])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_loaded.encode(\"डिसेंबर ह्या दिवशी श्रीकृष्णाने अर्जुनाला.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text_token = tokenizer_loaded.encode(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, seq_lengths, output_lengths = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.extend(tokens)\n",
    "seq_lengths.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for inference\n",
    "# input_ids.extend(tokens)\n",
    "# seq_lengths.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(input_ids, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_p(probs: torch.Tensor, p: float):\n",
    "    assert 0 <= p <= 1\n",
    "\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    return torch.gather(probs_idx, -1, next_token)\n",
    "\n",
    "\n",
    "def sample(logits: torch.Tensor, temperature: float, top_p: float):\n",
    "    if temperature > 0:\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        next_token = sample_top_p(probs, top_p)\n",
    "    else:\n",
    "        next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "    return next_token.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "logit = model.forward(input_ids=tensor, seqlens=seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 6600 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlogit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a Tensor with 6600 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 200])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 200])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.view(-1, logit.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logit.view(-1, logit.size(-1)), tensor.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.378178596496582"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_logits = torch.softmax(logit,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 200])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = sample(scaled_logits,temperature=0.5, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 64,   3, 165,  28,  91,  48,  67, 166,  70,  15, 125, 116,  88, 102,\n",
       "         88, 155, 171,  14, 103, 116, 134,  85, 150,  41,  43,  52, 179, 150,\n",
       "        125,  69, 181, 126,  62], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text = tokenizer_loaded.decode(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'े मोरासेविडिठे थिंवडॉमुफ मका व ल्रप् त्यां्यानेुरि्यीनष आरकाे'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text_token = tokenizer_loaded.encode(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.extend(tokens)\n",
    "seq_lengths[0] = seq_lengths[0]+len(predicted_text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.tensor(input_ids, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([66])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "(68, 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m logit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\my-llm\\mistral-src-commented\\mistral\\model.py:302\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, input_ids, seqlens, cache)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    298\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;66;03m# The concatenated tokens of the batch\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     seqlens: List[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;66;03m# The sequence length of each input in the batch (so we can understand which token belongs to which prompt)\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     cache: Optional[RotatingBufferCache] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 302\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_rank \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_pipeline_ranks \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;66;03m# ignore the intermediate activations as we'll get the final output from\u001b[39;00m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# the last stage\u001b[39;00m\n\u001b[0;32m    306\u001b[0m         outs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m    307\u001b[0m             h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, device\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mh\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    308\u001b[0m         )\n",
      "File \u001b[1;32md:\\my-llm\\mistral-src-commented\\mistral\\model.py:254\u001b[0m, in \u001b[0;36mTransformer.forward_partial\u001b[1;34m(self, input_ids, seqlens, cache)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mlen\u001b[39m(seqlens) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_batch_size\n\u001b[0;32m    252\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax batch size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got batch size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(seqlens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m (num_toks,) \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 254\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m(seqlens) \u001b[38;5;241m==\u001b[39m num_toks, (\u001b[38;5;28msum\u001b[39m(seqlens), num_toks)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;66;03m# Generate the attention mask based on the current stage: first pre-fill, subsequent pre-fill or token generation.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m     input_metadata \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget_input_metadata(seqlens)\n",
      "\u001b[1;31mAssertionError\u001b[0m: (68, 66)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "logit = model.forward(input_ids=tensor, seqlens=seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6331, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([86, 200])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0389e+00,  2.3724e-04,  3.0089e-01,  ..., -1.8550e-02,\n",
       "          9.4721e-01, -1.9433e+00],\n",
       "        [ 2.7117e-01, -5.8892e-01, -7.4425e-02,  ..., -5.8432e-01,\n",
       "          1.7085e-02, -7.2686e-02],\n",
       "        [-9.9749e-02,  7.0405e-01,  1.1654e-01,  ..., -1.3002e+00,\n",
       "         -1.7605e+00, -6.0589e-01],\n",
       "        ...,\n",
       "        [-1.4768e-01,  4.3750e-01,  5.2466e-01,  ...,  3.8173e-01,\n",
       "         -1.5919e-01, -7.7899e-01],\n",
       "        [-7.5604e-01, -7.6008e-01,  6.5404e-01,  ...,  3.9160e-02,\n",
       "         -5.8079e-01, -8.1597e-02],\n",
       "        [ 4.1231e-01, -6.0959e-01,  5.8402e-01,  ...,  4.3537e-01,\n",
       "         -2.5136e-02,  4.9881e-01]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.view(-1, logit.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = torch.softmax(logit, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(43., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0120, 0.0042, 0.0057,  ..., 0.0042, 0.0110, 0.0006],\n",
       "        [0.0054, 0.0023, 0.0038,  ..., 0.0023, 0.0042, 0.0038],\n",
       "        [0.0040, 0.0090, 0.0050,  ..., 0.0012, 0.0008, 0.0024],\n",
       "        ...,\n",
       "        [0.0033, 0.0060, 0.0065,  ..., 0.0057, 0.0033, 0.0018],\n",
       "        [0.0022, 0.0022, 0.0090,  ..., 0.0049, 0.0026, 0.0043],\n",
       "        [0.0065, 0.0023, 0.0077,  ..., 0.0067, 0.0042, 0.0071]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43, 200])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text_from_file(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "text = read_text_from_file('D:\\mr_dataset\\mr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174960962\n"
     ]
    }
   ],
   "source": [
    "print(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\mr_dataset\\mr.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_loaded.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer_loaded.padding_side = \"right\"\n",
    "#tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_chunk(filename):\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        block_size = 40\n",
    "        batch_size=8\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            #data = torch.tensor(tokenizer_loaded.encode(decoded_block), dtype=torch.long)\n",
    "            data = tokenizer_loaded.encode(decoded_block,padding='max_length' ,max_length=100)\n",
    "            # data = tokenizer_loaded.encode(decoded_block)\n",
    "            \n",
    "    return data #decoded_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(get_random_chunk(filename='D:\\mr_dataset\\mr.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 50, 184, 5, 11, 11, 11, 0, 49, 142, 48, 86, 36, 167, 42, 6, 29, 62, 20, 42, 28, 190, 6, 48, 71, 187, 61, 4, 74, 68, 145, 5, 0, 32, 8, 19, 6, 9, 89, 73, 129, 74, 42, 180, 9, 87, 81, 28, 0, 194, 190, 118, 79, 8, 5, 198, 32, 70, 7, 30, 3, 72]\n"
     ]
    }
   ],
   "source": [
    "print(get_random_chunk(filename='D:\\mr_dataset\\mr.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    block_size = 40\n",
    "    batch_size=8\n",
    "    input_ids, seq_lengths, output_ids= [], [],[]\n",
    "    \n",
    "    \n",
    "    # data = get_random_chunk(filename='D:\\mr_dataset\\mr.txt')\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    for _ in range(batch_size):\n",
    "        data = get_random_chunk(filename='D:\\mr_dataset\\mr.txt')\n",
    "        input_ids.extend(data[:block_size])\n",
    "        output_ids.extend(data[1:block_size+1])\n",
    "        seq_lengths.append(len(data[:block_size]))\n",
    "    # input_ids.extend([data[i:i+block_size] for i in ix])\n",
    "    # seq_lengths.append(len(input_ids))\n",
    "    x = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
    "   \n",
    "    # x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # for i in range(batch_size):\n",
    "    #     output_ids.extend(data[i+1:i+block_size+1])\n",
    "    # output_ids.extend([data[i+7:i+block_size+7] for i in ix])\n",
    "    y = torch.tensor(output_ids, dtype=torch.long).to(device)\n",
    "    \n",
    "    # y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # x, y = x.to(device), y.to(device)\n",
    "    #return [data[i:i+block_size] for i in ix],[data[i+7:i+block_size+7] for i in ix]\n",
    "\n",
    "    # x, y,seq_lengths = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True), seq_lengths.to(device)\n",
    "    # x, y,seq_lengths = x.to(device, non_blocking=True), y.to(device, non_blocking=True), seq_lengths.to(device)\n",
    "    return x,y ,seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,seq_lengths =get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320]), torch.Size([320]), [40, 40, 40, 40, 40, 40, 40, 40])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape , seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    x,y,seq_lengths = ge\n",
    "    logits = model(input_ids=x, seqlens=seq_lengths)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)),   y.view(-1), ignore_index=-1)\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 10]), torch.Size([8, 10]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = get_batch()\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids=x, seqlens=seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(logits.view(-1, logits.size(-1)),   y.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4894, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 1000#600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 200 # how many steps to warm up for\n",
    "lr_decay_iters = 1000#600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), weight_decay= weight_decay, lr=learning_rate, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.9e-05"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_lr(33)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('weights/my-lm_epochs-1000.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"PYTORCH_USE_CUDA_DSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 \n",
      "epoch: 1 \n",
      "epoch: 2 \n",
      "epoch: 3 \n",
      "epoch: 4 \n",
      "epoch: 5 \n",
      "epoch: 6 \n",
      "epoch: 7 \n",
      "epoch: 8 \n",
      "epoch: 9 \n",
      "epoch: 10 \n",
      "epoch: 11 \n",
      "epoch: 12 \n",
      "epoch: 13 \n",
      "epoch: 14 \n",
      "epoch: 15 \n",
      "epoch: 16 \n",
      "epoch: 17 \n",
      "epoch: 18 \n",
      "epoch: 19 \n",
      "epoch: 20 \n",
      "epoch: 21 \n",
      "epoch: 22 \n",
      "epoch: 23 \n",
      "epoch: 24 \n",
      "epoch: 25 \n",
      "epoch: 26 \n",
      "epoch: 27 \n",
      "epoch: 28 \n",
      "epoch: 29 \n",
      "epoch: 30 \n",
      "epoch: 31 \n",
      "epoch: 32 \n",
      "epoch: 33 \n",
      "epoch: 34 \n",
      "epoch: 35 \n",
      "epoch: 36 \n",
      "epoch: 37 \n",
      "epoch: 38 \n",
      "epoch: 39 \n",
      "epoch: 40 \n",
      "epoch: 41 \n",
      "epoch: 42 \n",
      "epoch: 43 \n",
      "epoch: 44 \n",
      "epoch: 45 \n",
      "epoch: 46 \n",
      "epoch: 47 \n",
      "epoch: 48 \n",
      "epoch: 49 \n",
      "epoch: 50 \n",
      "epoch: 51 \n",
      "epoch: 52 \n",
      "epoch: 53 \n",
      "epoch: 54 \n",
      "epoch: 55 \n",
      "epoch: 56 \n",
      "epoch: 57 \n",
      "epoch: 58 \n",
      "epoch: 59 \n",
      "epoch: 60 \n",
      "epoch: 61 \n",
      "epoch: 62 \n",
      "epoch: 63 \n",
      "epoch: 64 \n",
      "epoch: 65 \n",
      "epoch: 66 \n",
      "epoch: 67 \n",
      "epoch: 68 \n",
      "epoch: 69 \n",
      "epoch: 70 \n",
      "epoch: 71 \n",
      "epoch: 72 \n",
      "epoch: 73 \n",
      "epoch: 74 \n",
      "epoch: 75 \n",
      "epoch: 76 \n",
      "epoch: 77 \n",
      "epoch: 78 \n",
      "epoch: 79 \n",
      "epoch: 80 \n",
      "epoch: 81 \n",
      "epoch: 82 \n",
      "epoch: 83 \n",
      "epoch: 84 \n",
      "epoch: 85 \n",
      "epoch: 86 \n",
      "epoch: 87 \n",
      "epoch: 88 \n",
      "epoch: 89 \n",
      "epoch: 90 \n",
      "epoch: 91 \n",
      "epoch: 92 \n",
      "epoch: 93 \n",
      "epoch: 94 \n",
      "epoch: 95 \n",
      "epoch: 96 \n",
      "epoch: 97 \n",
      "epoch: 98 \n",
      "epoch: 99 \n",
      "epoch: 100 \n",
      "epoch: 101 \n",
      "epoch: 102 \n",
      "epoch: 103 \n",
      "epoch: 104 \n",
      "epoch: 105 \n",
      "epoch: 106 \n",
      "epoch: 107 \n",
      "epoch: 108 \n",
      "epoch: 109 \n",
      "epoch: 110 \n",
      "epoch: 111 \n",
      "epoch: 112 \n",
      "epoch: 113 \n",
      "epoch: 114 \n",
      "epoch: 115 \n",
      "epoch: 116 \n",
      "epoch: 117 \n",
      "epoch: 118 \n",
      "epoch: 119 \n",
      "epoch: 120 \n",
      "epoch: 121 \n",
      "epoch: 122 \n",
      "epoch: 123 \n",
      "epoch: 124 \n",
      "epoch: 125 \n",
      "epoch: 126 \n",
      "epoch: 127 \n",
      "epoch: 128 \n",
      "epoch: 129 \n",
      "epoch: 130 \n",
      "epoch: 131 \n",
      "epoch: 132 \n",
      "epoch: 133 \n",
      "epoch: 134 \n",
      "epoch: 135 \n",
      "epoch: 136 \n",
      "epoch: 137 \n",
      "epoch: 138 \n",
      "epoch: 139 \n",
      "epoch: 140 \n",
      "epoch: 141 \n",
      "epoch: 142 \n",
      "epoch: 143 \n",
      "epoch: 144 \n",
      "epoch: 145 \n",
      "epoch: 146 \n",
      "epoch: 147 \n",
      "epoch: 148 \n",
      "epoch: 149 \n",
      "epoch: 150 \n",
      "epoch: 151 \n",
      "epoch: 152 \n",
      "epoch: 153 \n",
      "epoch: 154 \n",
      "epoch: 155 \n",
      "epoch: 156 \n",
      "epoch: 157 \n",
      "epoch: 158 \n",
      "epoch: 159 \n",
      "epoch: 160 \n",
      "epoch: 161 \n",
      "epoch: 162 \n",
      "epoch: 163 \n",
      "epoch: 164 \n",
      "epoch: 165 \n",
      "epoch: 166 \n",
      "epoch: 167 \n",
      "epoch: 168 \n",
      "epoch: 169 \n",
      "epoch: 170 \n",
      "epoch: 171 \n",
      "epoch: 172 \n",
      "epoch: 173 \n",
      "epoch: 174 \n",
      "epoch: 175 \n",
      "epoch: 176 \n",
      "epoch: 177 \n",
      "epoch: 178 \n",
      "epoch: 179 \n",
      "epoch: 180 \n",
      "epoch: 181 \n",
      "epoch: 182 \n",
      "epoch: 183 \n",
      "epoch: 184 \n",
      "epoch: 185 \n",
      "epoch: 186 \n",
      "epoch: 187 \n",
      "epoch: 188 \n",
      "epoch: 189 \n",
      "epoch: 190 \n",
      "epoch: 191 \n",
      "epoch: 192 \n",
      "epoch: 193 \n",
      "epoch: 194 \n",
      "epoch: 195 \n",
      "epoch: 196 \n",
      "epoch: 197 \n",
      "epoch: 198 \n",
      "epoch: 199 \n",
      "epoch: 200 \n",
      "epoch: 201 \n",
      "epoch: 202 \n",
      "epoch: 203 \n",
      "epoch: 204 \n",
      "epoch: 205 \n",
      "epoch: 206 \n",
      "epoch: 207 \n",
      "epoch: 208 \n",
      "epoch: 209 \n",
      "epoch: 210 \n",
      "epoch: 211 \n",
      "epoch: 212 \n",
      "epoch: 213 \n",
      "epoch: 214 \n",
      "epoch: 215 \n",
      "epoch: 216 \n",
      "epoch: 217 \n",
      "epoch: 218 \n",
      "epoch: 219 \n",
      "epoch: 220 \n",
      "epoch: 221 \n",
      "epoch: 222 \n",
      "epoch: 223 \n",
      "epoch: 224 \n",
      "epoch: 225 \n",
      "epoch: 226 \n",
      "epoch: 227 \n",
      "epoch: 228 \n",
      "epoch: 229 \n",
      "epoch: 230 \n",
      "epoch: 231 \n",
      "epoch: 232 \n",
      "epoch: 233 \n",
      "epoch: 234 \n",
      "epoch: 235 \n",
      "epoch: 236 \n",
      "epoch: 237 \n",
      "epoch: 238 \n",
      "epoch: 239 \n",
      "epoch: 240 \n",
      "epoch: 241 \n",
      "epoch: 242 \n",
      "epoch: 243 \n",
      "epoch: 244 \n",
      "epoch: 245 \n",
      "epoch: 246 \n",
      "epoch: 247 \n",
      "epoch: 248 \n",
      "epoch: 249 \n",
      "epoch: 250 \n",
      "epoch: 251 \n",
      "epoch: 252 \n",
      "epoch: 253 \n",
      "epoch: 254 \n",
      "epoch: 255 \n",
      "epoch: 256 \n",
      "epoch: 257 \n",
      "epoch: 258 \n",
      "epoch: 259 \n",
      "epoch: 260 \n",
      "epoch: 261 \n",
      "epoch: 262 \n",
      "epoch: 263 \n",
      "epoch: 264 \n",
      "epoch: 265 \n",
      "epoch: 266 \n",
      "epoch: 267 \n",
      "epoch: 268 \n",
      "epoch: 269 \n",
      "epoch: 270 \n",
      "epoch: 271 \n",
      "epoch: 272 \n",
      "epoch: 273 \n",
      "epoch: 274 \n",
      "epoch: 275 \n",
      "epoch: 276 \n",
      "epoch: 277 \n",
      "epoch: 278 \n",
      "epoch: 279 \n",
      "epoch: 280 \n",
      "epoch: 281 \n",
      "epoch: 282 \n",
      "epoch: 283 \n",
      "epoch: 284 \n",
      "epoch: 285 \n",
      "epoch: 286 \n",
      "epoch: 287 \n",
      "epoch: 288 \n",
      "epoch: 289 \n",
      "epoch: 290 \n",
      "epoch: 291 \n",
      "epoch: 292 \n",
      "epoch: 293 \n",
      "epoch: 294 \n",
      "epoch: 295 \n",
      "epoch: 296 \n",
      "epoch: 297 \n",
      "epoch: 298 \n",
      "epoch: 299 \n",
      "epoch: 300 \n",
      "epoch: 301 \n",
      "epoch: 302 \n",
      "epoch: 303 \n",
      "epoch: 304 \n",
      "epoch: 305 \n",
      "epoch: 306 \n",
      "epoch: 307 \n",
      "epoch: 308 \n",
      "epoch: 309 \n",
      "epoch: 310 \n",
      "epoch: 311 \n",
      "epoch: 312 \n",
      "epoch: 313 \n",
      "epoch: 314 \n",
      "epoch: 315 \n",
      "epoch: 316 \n",
      "epoch: 317 \n",
      "epoch: 318 \n",
      "epoch: 319 \n",
      "epoch: 320 \n",
      "epoch: 321 \n",
      "epoch: 322 \n",
      "epoch: 323 \n",
      "epoch: 324 \n",
      "epoch: 325 \n",
      "epoch: 326 \n",
      "epoch: 327 \n",
      "epoch: 328 \n",
      "epoch: 329 \n",
      "epoch: 330 \n",
      "epoch: 331 \n",
      "epoch: 332 \n",
      "epoch: 333 \n",
      "epoch: 334 \n",
      "epoch: 335 \n",
      "epoch: 336 \n",
      "epoch: 337 \n",
      "epoch: 338 \n",
      "epoch: 339 \n",
      "epoch: 340 \n",
      "epoch: 341 \n",
      "epoch: 342 \n",
      "epoch: 343 \n",
      "epoch: 344 \n",
      "epoch: 345 \n",
      "epoch: 346 \n",
      "epoch: 347 \n",
      "epoch: 348 \n",
      "epoch: 349 \n",
      "epoch: 350 \n",
      "epoch: 351 \n",
      "epoch: 352 \n",
      "epoch: 353 \n",
      "epoch: 354 \n",
      "epoch: 355 \n",
      "epoch: 356 \n",
      "epoch: 357 \n",
      "epoch: 358 \n",
      "epoch: 359 \n",
      "epoch: 360 \n",
      "epoch: 361 \n",
      "epoch: 362 \n",
      "epoch: 363 \n",
      "epoch: 364 \n",
      "epoch: 365 \n",
      "epoch: 366 \n",
      "epoch: 367 \n",
      "epoch: 368 \n",
      "epoch: 369 \n",
      "epoch: 370 \n",
      "epoch: 371 \n",
      "epoch: 372 \n",
      "epoch: 373 \n",
      "epoch: 374 \n",
      "epoch: 375 \n",
      "epoch: 376 \n",
      "epoch: 377 \n",
      "epoch: 378 \n",
      "epoch: 379 \n",
      "epoch: 380 \n",
      "epoch: 381 \n",
      "epoch: 382 \n",
      "epoch: 383 \n",
      "epoch: 384 \n",
      "epoch: 385 \n",
      "epoch: 386 \n",
      "epoch: 387 \n",
      "epoch: 388 \n",
      "epoch: 389 \n",
      "epoch: 390 \n",
      "epoch: 391 \n",
      "epoch: 392 \n",
      "epoch: 393 \n",
      "epoch: 394 \n",
      "epoch: 395 \n",
      "epoch: 396 \n",
      "epoch: 397 \n",
      "epoch: 398 \n",
      "epoch: 399 \n",
      "epoch: 400 \n",
      "epoch: 401 \n",
      "epoch: 402 \n",
      "epoch: 403 \n",
      "epoch: 404 \n",
      "epoch: 405 \n",
      "epoch: 406 \n",
      "epoch: 407 \n",
      "epoch: 408 \n",
      "epoch: 409 \n",
      "epoch: 410 \n",
      "epoch: 411 \n",
      "epoch: 412 \n",
      "epoch: 413 \n",
      "epoch: 414 \n",
      "epoch: 415 \n",
      "epoch: 416 \n",
      "epoch: 417 \n",
      "epoch: 418 \n",
      "epoch: 419 \n",
      "epoch: 420 \n",
      "epoch: 421 \n",
      "epoch: 422 \n",
      "epoch: 423 \n",
      "epoch: 424 \n",
      "epoch: 425 \n",
      "epoch: 426 \n",
      "epoch: 427 \n",
      "epoch: 428 \n",
      "epoch: 429 \n",
      "epoch: 430 \n",
      "epoch: 431 \n",
      "epoch: 432 \n",
      "epoch: 433 \n",
      "epoch: 434 \n",
      "epoch: 435 \n",
      "epoch: 436 \n",
      "epoch: 437 \n",
      "epoch: 438 \n",
      "epoch: 439 \n",
      "epoch: 440 \n",
      "epoch: 441 \n",
      "epoch: 442 \n",
      "epoch: 443 \n",
      "epoch: 444 \n",
      "epoch: 445 \n",
      "epoch: 446 \n",
      "epoch: 447 \n",
      "epoch: 448 \n",
      "epoch: 449 \n",
      "epoch: 450 \n",
      "epoch: 451 \n",
      "epoch: 452 \n",
      "epoch: 453 \n",
      "epoch: 454 \n",
      "epoch: 455 \n",
      "epoch: 456 \n",
      "epoch: 457 \n",
      "epoch: 458 \n",
      "epoch: 459 \n",
      "epoch: 460 \n",
      "epoch: 461 \n",
      "epoch: 462 \n",
      "epoch: 463 \n",
      "epoch: 464 \n",
      "epoch: 465 \n",
      "epoch: 466 \n",
      "epoch: 467 \n",
      "epoch: 468 \n",
      "epoch: 469 \n",
      "epoch: 470 \n",
      "epoch: 471 \n",
      "epoch: 472 \n",
      "epoch: 473 \n",
      "epoch: 474 \n",
      "epoch: 475 \n",
      "epoch: 476 \n",
      "epoch: 477 \n",
      "epoch: 478 \n",
      "epoch: 479 \n",
      "epoch: 480 \n",
      "epoch: 481 \n",
      "epoch: 482 \n",
      "epoch: 483 \n",
      "epoch: 484 \n",
      "epoch: 485 \n",
      "epoch: 486 \n",
      "epoch: 487 \n",
      "epoch: 488 \n",
      "epoch: 489 \n",
      "epoch: 490 \n",
      "epoch: 491 \n",
      "epoch: 492 \n",
      "epoch: 493 \n",
      "epoch: 494 \n",
      "epoch: 495 \n",
      "epoch: 496 \n",
      "epoch: 497 \n",
      "epoch: 498 \n",
      "epoch: 499 \n",
      "epoch: 500 \n",
      "epoch: 501 \n",
      "epoch: 502 \n",
      "epoch: 503 \n",
      "epoch: 504 \n",
      "epoch: 505 \n",
      "epoch: 506 \n",
      "epoch: 507 \n",
      "epoch: 508 \n",
      "epoch: 509 \n",
      "epoch: 510 \n",
      "epoch: 511 \n",
      "epoch: 512 \n",
      "epoch: 513 \n",
      "epoch: 514 \n",
      "epoch: 515 \n",
      "epoch: 516 \n",
      "epoch: 517 \n",
      "epoch: 518 \n",
      "epoch: 519 \n",
      "epoch: 520 \n",
      "epoch: 521 \n",
      "epoch: 522 \n",
      "epoch: 523 \n",
      "epoch: 524 \n",
      "epoch: 525 \n",
      "epoch: 526 \n",
      "epoch: 527 \n",
      "epoch: 528 \n",
      "epoch: 529 \n",
      "epoch: 530 \n",
      "epoch: 531 \n",
      "epoch: 532 \n",
      "epoch: 533 \n",
      "epoch: 534 \n",
      "epoch: 535 \n",
      "epoch: 536 \n",
      "epoch: 537 \n",
      "epoch: 538 \n",
      "epoch: 539 \n",
      "epoch: 540 \n",
      "epoch: 541 \n",
      "epoch: 542 \n",
      "epoch: 543 \n",
      "epoch: 544 \n",
      "epoch: 545 \n",
      "epoch: 546 \n",
      "epoch: 547 \n",
      "epoch: 548 \n",
      "epoch: 549 \n",
      "epoch: 550 \n",
      "epoch: 551 \n",
      "epoch: 552 \n",
      "epoch: 553 \n",
      "epoch: 554 \n",
      "epoch: 555 \n",
      "epoch: 556 \n",
      "epoch: 557 \n",
      "epoch: 558 \n",
      "epoch: 559 \n",
      "epoch: 560 \n",
      "epoch: 561 \n",
      "epoch: 562 \n",
      "epoch: 563 \n",
      "epoch: 564 \n",
      "epoch: 565 \n",
      "epoch: 566 \n",
      "epoch: 567 \n",
      "epoch: 568 \n",
      "epoch: 569 \n",
      "epoch: 570 \n",
      "epoch: 571 \n",
      "epoch: 572 \n",
      "epoch: 573 \n",
      "epoch: 574 \n",
      "epoch: 575 \n",
      "epoch: 576 \n",
      "epoch: 577 \n",
      "epoch: 578 \n",
      "epoch: 579 \n",
      "epoch: 580 \n",
      "epoch: 581 \n",
      "epoch: 582 \n",
      "epoch: 583 \n",
      "epoch: 584 \n",
      "epoch: 585 \n",
      "epoch: 586 \n",
      "epoch: 587 \n",
      "epoch: 588 \n",
      "epoch: 589 \n",
      "epoch: 590 \n",
      "epoch: 591 \n",
      "epoch: 592 \n",
      "epoch: 593 \n",
      "epoch: 594 \n",
      "epoch: 595 \n",
      "epoch: 596 \n",
      "epoch: 597 \n",
      "epoch: 598 \n",
      "epoch: 599 \n",
      "epoch: 600 \n",
      "epoch: 601 \n",
      "epoch: 602 \n",
      "epoch: 603 \n",
      "epoch: 604 \n",
      "epoch: 605 \n",
      "epoch: 606 \n",
      "epoch: 607 \n",
      "epoch: 608 \n",
      "epoch: 609 \n",
      "epoch: 610 \n",
      "epoch: 611 \n",
      "epoch: 612 \n",
      "epoch: 613 \n",
      "epoch: 614 \n",
      "epoch: 615 \n",
      "epoch: 616 \n",
      "epoch: 617 \n",
      "epoch: 618 \n",
      "epoch: 619 \n",
      "epoch: 620 \n",
      "epoch: 621 \n",
      "epoch: 622 \n",
      "epoch: 623 \n",
      "epoch: 624 \n",
      "epoch: 625 \n",
      "epoch: 626 \n",
      "epoch: 627 \n",
      "epoch: 628 \n",
      "epoch: 629 \n",
      "epoch: 630 \n",
      "epoch: 631 \n",
      "epoch: 632 \n",
      "epoch: 633 \n",
      "epoch: 634 \n",
      "epoch: 635 \n",
      "epoch: 636 \n",
      "epoch: 637 \n",
      "epoch: 638 \n",
      "epoch: 639 \n",
      "epoch: 640 \n",
      "epoch: 641 \n",
      "epoch: 642 \n",
      "epoch: 643 \n",
      "epoch: 644 \n",
      "epoch: 645 \n",
      "epoch: 646 \n",
      "epoch: 647 \n",
      "epoch: 648 \n",
      "epoch: 649 \n",
      "epoch: 650 \n",
      "epoch: 651 \n",
      "epoch: 652 \n",
      "epoch: 653 \n",
      "epoch: 654 \n",
      "epoch: 655 \n",
      "epoch: 656 \n",
      "epoch: 657 \n",
      "epoch: 658 \n",
      "epoch: 659 \n",
      "epoch: 660 \n",
      "epoch: 661 \n",
      "epoch: 662 \n",
      "epoch: 663 \n",
      "epoch: 664 \n",
      "epoch: 665 \n",
      "epoch: 666 \n",
      "epoch: 667 \n",
      "epoch: 668 \n",
      "epoch: 669 \n",
      "epoch: 670 \n",
      "epoch: 671 \n",
      "epoch: 672 \n",
      "epoch: 673 \n",
      "epoch: 674 \n",
      "epoch: 675 \n",
      "epoch: 676 \n",
      "epoch: 677 \n",
      "epoch: 678 \n",
      "epoch: 679 \n",
      "epoch: 680 \n",
      "epoch: 681 \n",
      "epoch: 682 \n",
      "epoch: 683 \n",
      "epoch: 684 \n",
      "epoch: 685 \n",
      "epoch: 686 \n",
      "epoch: 687 \n",
      "epoch: 688 \n",
      "epoch: 689 \n",
      "epoch: 690 \n",
      "epoch: 691 \n",
      "epoch: 692 \n",
      "epoch: 693 \n",
      "epoch: 694 \n",
      "epoch: 695 \n",
      "epoch: 696 \n",
      "epoch: 697 \n",
      "epoch: 698 \n",
      "epoch: 699 \n",
      "epoch: 700 \n",
      "epoch: 701 \n",
      "epoch: 702 \n",
      "epoch: 703 \n",
      "epoch: 704 \n",
      "epoch: 705 \n",
      "epoch: 706 \n",
      "epoch: 707 \n",
      "epoch: 708 \n",
      "epoch: 709 \n",
      "epoch: 710 \n",
      "epoch: 711 \n",
      "epoch: 712 \n",
      "epoch: 713 \n",
      "epoch: 714 \n",
      "epoch: 715 \n",
      "epoch: 716 \n",
      "epoch: 717 \n",
      "epoch: 718 \n",
      "epoch: 719 \n",
      "epoch: 720 \n",
      "epoch: 721 \n",
      "epoch: 722 \n",
      "epoch: 723 \n",
      "epoch: 724 \n",
      "epoch: 725 \n",
      "epoch: 726 \n",
      "epoch: 727 \n",
      "epoch: 728 \n",
      "epoch: 729 \n",
      "epoch: 730 \n",
      "epoch: 731 \n",
      "epoch: 732 \n",
      "epoch: 733 \n",
      "epoch: 734 \n",
      "epoch: 735 \n",
      "epoch: 736 \n",
      "epoch: 737 \n",
      "epoch: 738 \n",
      "epoch: 739 \n",
      "epoch: 740 \n",
      "epoch: 741 \n",
      "epoch: 742 \n",
      "epoch: 743 \n",
      "epoch: 744 \n",
      "epoch: 745 \n",
      "epoch: 746 \n",
      "epoch: 747 \n",
      "epoch: 748 \n",
      "epoch: 749 \n",
      "epoch: 750 \n",
      "epoch: 751 \n",
      "epoch: 752 \n",
      "epoch: 753 \n",
      "epoch: 754 \n",
      "epoch: 755 \n",
      "epoch: 756 \n",
      "epoch: 757 \n",
      "epoch: 758 \n",
      "epoch: 759 \n",
      "epoch: 760 \n",
      "epoch: 761 \n",
      "epoch: 762 \n",
      "epoch: 763 \n",
      "epoch: 764 \n",
      "epoch: 765 \n",
      "epoch: 766 \n",
      "epoch: 767 \n",
      "epoch: 768 \n",
      "epoch: 769 \n",
      "epoch: 770 \n",
      "epoch: 771 \n",
      "epoch: 772 \n",
      "epoch: 773 \n",
      "epoch: 774 \n",
      "epoch: 775 \n",
      "epoch: 776 \n",
      "epoch: 777 \n",
      "epoch: 778 \n",
      "epoch: 779 \n",
      "epoch: 780 \n",
      "epoch: 781 \n",
      "epoch: 782 \n",
      "epoch: 783 \n",
      "epoch: 784 \n",
      "epoch: 785 \n",
      "epoch: 786 \n",
      "epoch: 787 \n",
      "epoch: 788 \n",
      "epoch: 789 \n",
      "epoch: 790 \n",
      "epoch: 791 \n",
      "epoch: 792 \n",
      "epoch: 793 \n",
      "epoch: 794 \n",
      "epoch: 795 \n",
      "epoch: 796 \n",
      "epoch: 797 \n",
      "epoch: 798 \n",
      "epoch: 799 \n",
      "epoch: 800 \n",
      "epoch: 801 \n",
      "epoch: 802 \n",
      "epoch: 803 \n",
      "epoch: 804 \n",
      "epoch: 805 \n",
      "epoch: 806 \n",
      "epoch: 807 \n",
      "epoch: 808 \n",
      "epoch: 809 \n",
      "epoch: 810 \n",
      "epoch: 811 \n",
      "epoch: 812 \n",
      "epoch: 813 \n",
      "epoch: 814 \n",
      "epoch: 815 \n",
      "epoch: 816 \n",
      "epoch: 817 \n",
      "epoch: 818 \n",
      "epoch: 819 \n",
      "epoch: 820 \n",
      "epoch: 821 \n",
      "epoch: 822 \n",
      "epoch: 823 \n",
      "epoch: 824 \n",
      "epoch: 825 \n",
      "epoch: 826 \n",
      "epoch: 827 \n",
      "epoch: 828 \n",
      "epoch: 829 \n",
      "epoch: 830 \n",
      "epoch: 831 \n",
      "epoch: 832 \n",
      "epoch: 833 \n",
      "epoch: 834 \n",
      "epoch: 835 \n",
      "epoch: 836 \n",
      "epoch: 837 \n",
      "epoch: 838 \n",
      "epoch: 839 \n",
      "epoch: 840 \n",
      "epoch: 841 \n",
      "epoch: 842 \n",
      "epoch: 843 \n",
      "epoch: 844 \n",
      "epoch: 845 \n",
      "epoch: 846 \n",
      "epoch: 847 \n",
      "epoch: 848 \n",
      "epoch: 849 \n",
      "epoch: 850 \n",
      "epoch: 851 \n",
      "epoch: 852 \n",
      "epoch: 853 \n",
      "epoch: 854 \n",
      "epoch: 855 \n",
      "epoch: 856 \n",
      "epoch: 857 \n",
      "epoch: 858 \n",
      "epoch: 859 \n",
      "epoch: 860 \n",
      "epoch: 861 \n",
      "epoch: 862 \n",
      "epoch: 863 \n",
      "epoch: 864 \n",
      "epoch: 865 \n",
      "epoch: 866 \n",
      "epoch: 867 \n",
      "epoch: 868 \n",
      "epoch: 869 \n",
      "epoch: 870 \n",
      "epoch: 871 \n",
      "epoch: 872 \n",
      "epoch: 873 \n",
      "epoch: 874 \n",
      "epoch: 875 \n",
      "epoch: 876 \n",
      "epoch: 877 \n",
      "epoch: 878 \n",
      "epoch: 879 \n",
      "epoch: 880 \n",
      "epoch: 881 \n",
      "epoch: 882 \n",
      "epoch: 883 \n",
      "epoch: 884 \n",
      "epoch: 885 \n",
      "epoch: 886 \n",
      "epoch: 887 \n",
      "epoch: 888 \n",
      "epoch: 889 \n",
      "epoch: 890 \n",
      "epoch: 891 \n",
      "epoch: 892 \n",
      "epoch: 893 \n",
      "epoch: 894 \n",
      "epoch: 895 \n",
      "epoch: 896 \n",
      "epoch: 897 \n",
      "epoch: 898 \n",
      "epoch: 899 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#set lr with deacy_fn for each iter\u001b[39;00m\n\u001b[0;32m      6\u001b[0m lr \u001b[38;5;241m=\u001b[39m get_lr(iter_num) \u001b[38;5;28;01mif\u001b[39;00m decay_lr \u001b[38;5;28;01melse\u001b[39;00m learning_rate\n\u001b[1;32m----> 7\u001b[0m x,y,seq_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mx, seqlens\u001b[38;5;241m=\u001b[39mseq_lengths)\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),   y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     seq_lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(data[:block_size]))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# input_ids.extend([data[i:i+block_size] for i in ix])\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# seq_lengths.append(len(input_ids))\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# x = torch.stack([data[i:i+block_size] for i in ix])\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# for i in range(batch_size):\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#     output_ids.extend(data[i+1:i+block_size+1])\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# output_ids.extend([data[i+7:i+block_size+7] for i in ix])\u001b[39;00m\n\u001b[0;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(output_ids, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iter_num= 0\n",
    "while True:\n",
    "    total_loss=0\n",
    "    \n",
    "    #set lr with deacy_fn for each iter\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    x,y,seq_lengths = get_batch()\n",
    "    logits = model(input_ids=x, seqlens=seq_lengths)\n",
    "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)),   y.view(-1), ignore_index=-1)\n",
    "    # print(f'loss: {loss}')\n",
    "    # total_loss += loss.item()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    # print(f\"epoch: {iter_num:.0f} | Training_loss: {loss:.5f}\")\n",
    "    print(f\"epoch: {iter_num:.0f} \")\n",
    "    iter_num+=1\n",
    "\n",
    "    if iter_num>max_iters:\n",
    "        break\n",
    "\n",
    "# 3. Save the model state dict\n",
    "MODEL_SAVE_PATH = f'weights/my-lm_epochs-{max_iters}.pth'\n",
    "print(f'Saving model to: {MODEL_SAVE_PATH}')\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "           f=MODEL_SAVE_PATH)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
